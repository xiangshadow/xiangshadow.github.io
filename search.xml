<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Scrapy搭建爬虫项目]]></title>
    <url>%2F2018%2F09%2F13%2Fscrapydajianxiangmu%2F</url>
    <content type="text"><![CDATA[1.输入workon命令，进入已经设置好的一个虚拟环境。 2.安装scrapy框架:pip install scrapy 3.遇到如下报错信息，进入https://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy 下载Twisted库的wheel文件 4.安装Twisted 5.执行scrapy startproject 项目名称 5.using template director：是scrapy内置的一个框架模板 6.进入到项目文件夹中 7.爬虫项目文件介绍 8.通过cmd爬虫项目命令：scrapy crawl baidu(爬虫名称) 9.执行以上命令会报错只需要安装pip install pypiwin32即可解决然后再执行命令：：scrapy crawl baidu(爬虫名称)]]></content>
      <tags>
        <tag>学习心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python虚拟环境的安装和配置(windows)]]></title>
    <url>%2F2018%2F09%2F12%2Fxunihuanjing%2F</url>
    <content type="text"><![CDATA[1.下载virtualenvwrapper-win 包在黑窗中输入pip install virtualenvwrapper-win123C:\Users\Administrator&gt;pip install virtualenvwrapper-winRequirement already satisfied: virtualenvwrapper-win in c:\programdata\anaconda3\lib\site-packages (1.2.5)Requirement already satisfied: virtualenv in c:\programdata\anaconda3\lib\site-packages (from virtualenvwrapper-win) (16.0.0) 2.输入workon命令查看是否可用123Pass a name to activate one of the following virtualenvs:==============================================================================找不到文件 3.使用virtualenvwrapper创建虚拟环境在黑窗中输入mkvirtualenv py3scrapy (py3scrapy为虚拟环境名)1234C:\Users\Administrator&gt;mkvirtualenv py4scrapyUsing base prefix 'c:\\programdata\\anaconda3'New python executable in D:\Envs\py4scrapy\Scripts\python.exeInstalling setuptools, pip, wheel...done. 默认放在C:\Users\Administrator\Envs目录中可以修改存放的路径：找到系统环境变量，编辑系统变量,添加WORKON_HOME为变量名,D:\ENVS为变量值运行workon，目录中没有虚拟环境了，因为默认目录已经改变，可以将之前的虚拟环境拷贝到新目录下 新建一个虚拟环境，完成后自动进入该虚拟环境 以后再进入虚拟环境，就不需要记住安装路径了直接使用以下命令：列出虚拟环境列表：workon新建虚拟环境：mkvirtualenv [虚拟环境名称]启动/切换虚拟环境：workon [虚拟环境名称]离开虚拟环境：deactivate]]></content>
      <tags>
        <tag>学习心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[proxy_pool]]></title>
    <url>%2F2018%2F09%2F12%2Fproxy-pool%2F</url>
    <content type="text"><![CDATA[代理IP:1.免费的代理IP,可用IP少,时效性较短,大部分的代理IP可能.2.收费的代理IP,讯代理.3.爬取国外的网站,VPN代理服务器. pxory_pool:1.它将国内的代理IP网站都进行了爬取;2.代理IP爬取完毕之后,会进行金策,可用的IP会保存到数据库redis中;3.会定期将数据库中的代理IP拿出来检测,失效的IP从数据库中删除;4.支持扩展 附上proxy_pool的github链接:https://github.com/jhao104/proxy_pool]]></content>
      <tags>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[装了Chorme不用这个扩展,还不如去用IE6]]></title>
    <url>%2F2018%2F09%2F06%2Fa%2F</url>
    <content type="text"><![CDATA[这年头神器这个词已经烂大街了，但是，油猴的确配得上这个词，如果让我选择 Chrome 浏览器上只能保留一个扩展，那必须是油猴插件！这是一个可以提升上网幸福感的神级工具！说的这么邪乎？那我来告诉你这只油猴子都能干些什么吧。简单粗暴，直接上图！………..好吧,因为没有备案,所以图床还不能用…那就简单来讲讲吧,大致来说能干的事情非常的多,比如能看VIP视频、能下知网论文、磁链在线看、网盘免输密……等等,还有很多你想不到的功能.这么逆天的神器到底是个什么东西？它其实是一个脚本管理器，可以为你的浏览器加载各种各样有趣、实用、脑洞大开的脚本，对你正在访问的指定网站按照你的需求进行修改，从而让你的浏览器前所未有地强大。这款插件除了适用于 Chrome ,还支持 Microsoft Edge，Safari，Opera ，Firefox，以及使用 Chromium 内核的国产浏览器（比如QQ、猎豹、360、百度等），在 http://tampermonkey.net 这里你可以找到对应的油猴版本下载。我个人比较喜欢用Chrome,所以就以Chrome 为例讲解啦。 先安装油猴插件，最新版本号是4.4—— 1、可以科学上网的同学建议在线安装，去谷歌商店搜索 Tampermonkey 下载安装。 2、如果没有科学上网的条件，可以去 www.crx4chrome.com/crx/755/ 下载到本地，然后离线安装。顺便提一句，crx4chrome 这个站可以无需科学上网就能下载到最新版本的 Chrome 扩展，建议收藏。 安装成功之后，你会在 Chrome 的扩展栏看到一个黑色图标，证明油猴已经安装成功。 安装就这么简单，那这时候是不是就可以使用上边的那些神奇功能呢？ 还不行，安装好油猴插件相当于你手里有了一把枪，还要有子弹才能发挥枪的威力。 再送你一个丰富的弹药库，https://greasyfork.org/zh-CN/scripts，长枪短炮，应有尽有，刚才那些功能强大的脚本都来自于这个网站。 油猴上好玩好用的脚本实在是太多了，大家可以去自己慢慢淘，总有一款适合自己.]]></content>
      <tags>
        <tag>福利专区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy框架自定义pipeline]]></title>
    <url>%2F2018%2F09%2F04%2Ftest1%2F</url>
    <content type="text"><![CDATA[自定义pipeline首先,需要我们在pipeline.py文件中自定义.需要注意的是自定义的每一个pipeline必须是一个独立的Python类,也就是继承于object.就可以定义pipeline类,定义自己想要使用的储存方法. 演示保存为csv12345678910111213class NovelSpiderCsvPipeline(object): def open_spider(self,spider): self.f=open('novel,csv','w',encoding='utf-8',newline='') self.writer=csv.DictWriter(self.f,fieldnames=["novel_name","novel_info","novel_type","novel_author"]) self.writer.writeheader() def process_item(self,item,spider): item_dict=dict(item) self.writer.writerow(item_dict) return item def close_spider(self,spider): self.f.close() 然后在pipeline写好代码之后,不是就完成了,还有一些步骤要做.自定义的pipeline必须在settings中的ITEM_PIPELINES里面启用,否则该pipeline不会生效1ITEM_PIPELINES = &#123;'NovelSpider.pipelines.NovelSpiderCsvPipeline': 301&#125; 最后运行就可以了]]></content>
      <tags>
        <tag>学习心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客简介]]></title>
    <url>%2F2018%2F09%2F03%2Ftest%2F</url>
    <content type="text"><![CDATA[欢迎光临本博客的所有朋友们! 这是个普通空间，在这里故事不会被渲染，因为它来源于生活。 以前经常发表一些帖子或者通过聊天软件来表达自己的想法,可是都是零散的和杂乱的. 现在创建了一个个人博客,我可以把自己以前的和每天激发的一些想法或者感受放在自己的博客上,每次在写文章的时候,可能又会产生新的想法.虽然一些想法一些思考只是些皮毛,没有什么深度,但是当下笔去写的时候每次都会对某个小小的问题有了跟多的一点点思考.生活中每天的一个小小的事情都会引起人们的思考,甚至是和朋友的聊天中一个小小的火花迸发. 督促自己努力,把一时的想法变成观点,争取在声明中的每天里留下点什么,这也是写博客的目的之一.这也就能督促自己每一天不要浑浑噩噩,时光流逝无痕无声无息,写博客也是在自己的每一天留下了一道浅浅的思考和划痕.]]></content>
      <tags>
        <tag>生活笔记</tag>
      </tags>
  </entry>
</search>
