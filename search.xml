<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[scrapyd部署爬虫项目]]></title>
    <url>%2F2018%2F09%2F27%2Fbushupachong%2F</url>
    <content type="text"><![CDATA[它就相当于是一个服务器，用于将自己本地的爬虫代码，打包上传到服务器上，让这个爬虫在服务器上运行，可以实现对爬虫的远程管理。(远程启动爬虫，远程关闭爬虫，远程查看爬虫的一些日志。) 1.Scrapd的安装。Pip install scrapyd 2.如何将本地的爬虫项目Deploying(打包)，上传至scrapyd这个服务中。a&gt;scrapyd提供了一个客户端工具，就是scrapyd-client，使用这个工具对scrapyd这个服务进行操作，比如：向scrapyd服务打包上传项目。scrapyd-client类似于redis-cli.exe、mongodb数据库的client。scrapyd-client下载地址：https://github.com/scrapy/scrapyd-client b&gt;Pip install scrapyd-client==1.2.0a1 注意：服务端scrapyd(==1.2)和客户端scrapyd-client(==1.1)安装的版本一定要保持一致。 3.上述服务和客户端安装好之后，可以启动scrapyd这个服务了，服务启动之后，不要关闭。 访问127.0.0.1:6800,出现以下页面表示成功启动scrapyd服务： 4&gt;配置爬虫项目，完成以后，再通过addversion.json进行打包。 修改scrapy.cfg文件： 5&gt;上述的scrapyd服务窗口cmd不要关闭，再新打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。a&gt;进入项目根目录，然后输入scrapyd-deploy命令，查看scrapyd-client客户端命令能否正常使用； b&gt;查看当前可用于打包上传的爬虫项目； c&gt;使用scrapyd-deploy命令打包上传项目；命令：Scrapyd-deploy bole -p jobbolespider参数：Status: “ok”/”error” 项目上传状态Project: 上传的项目名称Version: 项目的版本号，值是时间戳Spiders: 项目Project包含的爬虫个数 d&gt;通过API接口，查看已经上传至scrapyd服务的项目；命令：curl http://localhost:6800/listprojects.json键值：Projects: [] 所有已经上传的爬虫项目，都会显示在这个列表中。 e&gt;通过API接口，查看某一个项目中的所有爬虫名称；命令：curl http://localhost:6800/listspiders.json?project=jobbolespider 注意：如果项目上传失败，需要先将爬虫项目中打包生成的文件删除(build、project.egg-info、setup.py)，然后再重新打包上传。 f&gt;通过API接口，启动爬虫项目；命令：curl http://localhost:6800/schedule.json -d project=爬虫项目名称 -d spider=项目中某一个爬虫名称键值：Jobid: 是根据项目(jobbolespider)和爬虫(bole)生成的一个id，将来用于取消爬虫任务。 g&gt;如果上传的项目无法运行，在本地调整代码以后，需要重新打包上传。将失效的项目删除。命令：curl http://localhost:6800/delproject.json -d project=jobbolespider h&gt;通过API接口，取消爬虫任务；参数：Jobid：启动爬虫的时候分配的]]></content>
      <tags>
        <tag>学习心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式爬虫的配置]]></title>
    <url>%2F2018%2F09%2F26%2Ffenbushi%2F</url>
    <content type="text"><![CDATA[分布式爬虫：将一个项目拷贝到多台电脑上，同时爬取数据。 1. 必须保证所有电脑上的代码是相同的配置。 2. 在其中一台电脑上启动 redis 和 MySQL 的数据库服务。 3. 同时将所有的爬虫项目运行起来。 4. 在启动 redis 和 MySQL 的电脑上， 向 redis 中添加起始的 url。 q = queue()url = q.get() # 如果队列是空的，那么 get() 方法会一直阻塞，直到能取到 url，才会继续向下执行。 单机爬虫：一台电脑运行一个项目。去重采用了 set() 和 queue()，但是这两个都是在内存中存在的。1 &gt; 其他电脑是无法获取另外一台电脑内存中的数据的。2 &gt; 程序终止，内存消失。 分布式问题：1. 多台电脑如何统一的对 URL 进行去重？ 2. 多台电脑之间如何共用相同的队列？多台电脑获取的 request，如何在多台电脑之间进行同步？ 3. 多台电脑运行同一个爬虫项目，如果有机器爬虫意外终止，如何保证可以继续从队列中获取新的 request，而不是从头开始爬取？ 前两个问题可以基于 Redis 实现。相当于将 set() 和 queue() 从 scrapy 框架中抽离出来，将其保存在一个公共的平台中（Redis）。第三个问题：scrapy_redis 已经实现了，重启爬虫不会从头开始重新爬取，而是会继续从队列中获取 request。不用担心爬虫意外终止。 scrapy_redis 第三方库实现分布的部署：分布式爬虫：只需要在众多电脑中，选择其中一台开启 redis 服务，目的就是在 redis 中创建公用的 queue 和公用的 set，然后剩余电脑只需要连接 redis 服务即可，剩余电脑不需要开启 redis-server 服务。 步骤：1 &gt; 在虚拟环境中安装 pip install redis2 &gt; 去 github 上搜索 scrapy_redis 库，解压，保存到项目根目录下。根据提供的用例，配置我们的项目，大致三部分： 1.settings.py 文件； # 配置 scrapy_redis 第三方库 # 所有电脑配置调度器，这个调度器重写了 scrapy 框架内置的调度器。 SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot; # 所有电脑配置去重， 这个也是重写了 scrapy 内置的去重。 DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot; 1234# 所有电脑配置redis的连接地址，设置局域网IP或者公网IP，保证所有电脑都能连接到同一个redis。6379是redis的默认端口号。# redis数据库默认只允许本地连接localhost，如需配置远程连接，需要修改redis的配置文件。修改完成以后，要重启redis-server服务。# 1. (A B C) 如果用的是局域网部署的分布式，选择其中一台电脑(A)开启redis-server服务，REDIS_URL就配置成A电脑的局域网IP地址。如果B电脑开启redis-server-&gt;REDIS_URL B电脑的IP地址。# 2. (A B C) 如果用的是公网IP(阿里云)，那么所有电脑都不需要开启redis-server服务，只需要将REDIS_URL的主机地址配置成公网IP即可。 # myroot: 自定义的 redis 链接（可不写）。IP：开启 redis-server 服务的这台电脑的 IP REDIS_URL = &apos;redis://myroot:@192.168.40.217:6379&apos; # item_pipelines可以配置，也可以不用配置。如果配置的话：所有下载的item，除了会被保存在数据库MySQL中，还会被保存在Redis数据库中。没有配置：所有的item不会存储在Redis数据库中 ITEM_PIPELINES = { &apos;scrapy_redis.pipelines.RedisPipeline&apos;: 300 } 1234567891011# 配置HOST为局域网IP，或者公网IPMYSQL_HOST = '192.168.70.246'MYSQL_DB = 'jobbole'MYSQL_USER = 'myroot'MYSQL_PASSWD = '123456'MYSQL_CHARSET = 'utf8'MYSQL_PORT = 3306 # 配置scrapy_redis第三方库# 所有电脑配置调度器，这个调度器重写了scrapy框架内置的调度器。SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 所有电脑配置去重， 这个也是重写了scrapy内置的去重。DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 所有电脑配置redis的连接地址，设置局域网IP或者公网IP，保证所有电脑都能连接到同一个redis。6379是redis的默认端口号。# redis数据库默认只允许本地连接localhost，如需配置远程连接，需要修改redis的配置文件。修改完成以后，要重启redis-server服务。 # 1. (A B C) 如果用的是局域网部署的分布式，选择其中一台电脑(A)开启redis-server服务，REDIS_URL就配置成A电脑的局域网IP地址。如果B电脑开启redis-server-&gt;REDIS_URL B电脑的IP地址。# 2. (A B C) 如果用的是公网IP(阿里云)，那么所有电脑都不需要开启redis-server服务，只需要将REDIS_URL的主机地址配置成公网IP即可。 REDIS_URL = 'redis://@192.168.70.246:6379' # 可以配置，也可以不用配置。如果配置的话：所有下载的item，除了会被保存在数据库MySQL中，还会被保存在Redis数据库中。没有配置：所有的item不会存储在Redis数据库中。# ITEM_PIPELINES = &#123;# 'scrapy_redis.pipelines.RedisPipeline': 300# &#125; 2.jobbole.py 文件； from scrapy_redis.spiders import RedisSpider class JobboleSpider(RedisSpider): name = &apos;jobbole&apos; allowed_domains = [&apos;jobbole.com&apos;] # start_urls = [&apos;http://blog.jobbole.com/all-posts/&apos;] # 添加键 redis_key = &apos;jobbole:start_urls&apos; 1234567891011121314import scrapy,refrom ..items import JobbolespiderItemfrom urllib.parse import urljoinfrom scrapy.loader import ItemLoaderfrom scrapy_redis.spiders import RedisSpiderclass BoleSpider(RedisSpider): name = 'bole' allowed_domains = ['jobbole.com'] # start_urls = ['http://blog.jobbole.com/all-posts/'] # 分布式就不需要再设置起始的url，需要通过redis进行添加起始的url。起始的url添加到哪去了？被添加到了公共队列queue中。让多台机器中的其中一台从公共的reids队列queue中，获取起始的url，并对这个起始的url进行请求和解析，获取更多的url，然后将所有的url构造成Request，还放入公共队列queue中，让其他机器获取这些Request请求。 redis_key = 'jobbole:start_urls' 3. 有关数据库部分；首先要把 MySQL 添加到环境变量中 第一步：通过 mysql -uroot -p 登录 MySQL 服务。 第二步：通过 grant all privileges on *.* to &apos;myroot&apos;@&apos;%&apos; identified by &apos;123456&apos;;(注意一定要带上分号)。 # *.* 表示所有数据库中的所有表，都能够被远程连接 # &apos;%&apos; 表示任意 IP 都可以进行链接 # &apos;myroot&apos; 具有远程链接权限的用户名，自定义。之后就使用这个 User 进行链接数据库 mysql-&gt;grant all privileges on *.* to &apos;myroot&apos;@&apos;%&apos; identified by &apos;123456&apos;; 回车即可 第三步：再去修改爬虫项目（settings.py）中有关数据库的配置。 登录 MySQL 服务 mysql -uroot -p 创建远程连接新用户： create user &quot;user&quot;@&quot;%&quot; identified by &quot;123456&quot;; 给用户分配所有数据库的访问权限： GRANT ALL privileges ON *.* TO &apos;user&apos;@&apos;%&apos;; 1234567 # 配置数据库MYSQL_HOST = '192.168.53.64'MYSQL_DB = 'jobbole'MYSQL_USER = 'myuser'MYSQL_PASSWD = '123456'MYSQL_PORT = 3306MYSQL_CHARSET = 'utf8' 然后在数据库中新建连接，连接名随便起，主机名或 IP 地址要与 settings.py 中配置的一致，即开启 redis-server 服务的这台电脑 的 IP，用户名和密码与 settings.py 一致。 3 &gt; 将配置好的项目，拷贝到不同的机器中； 4 &gt; 选择其中一台机器，开启 redis-server 服务，并修改 redis.windows.conf 配置文件： #配置远程 IP 地址，供其他的电脑进行连接 redis bind: (当前电脑 IP) 192.168.40.217 # 关闭 redis 保护模式protected-mode: no 在修改redis.windows.conf配置文件以后，必须将 &quot;Redis&quot; 服务从计算机-管理中删除，然后重新根据修改后redis.windows.conf配置文件进行注册新的服务，这样修改的配置文件才生效。 删除服务: src detele Redis 启用服务: redis-server.exe --service-install redis.windows.conf --loglevel verbose 5 &gt; 其中一台电脑启动 redis-server 服务 6 &gt; 让所有爬虫项目都运行起来，由于没有起始的 url，所有爬虫会暂时处于停滞状态 7 &gt; 所有爬虫都启动之后，部署 redis-server 服务的电脑，通过命令 redis-cli lpush jobbole:start_urls http://blog.jobbole.com/all-posts/ 向 redis 的 queue 中添加起始的 url 输入:redis-cli.exe -h 192.168.... 再添加: lpush jobbole:start_urls http://blog.jobbole.com/all-posts/ 8 &gt; 所有爬虫开始运行，爬取数据，同时所有的数据都会保存到该爬虫所连接的远程数据库以及远程 redis 中]]></content>
      <tags>
        <tag>技术分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy搭建爬虫项目]]></title>
    <url>%2F2018%2F09%2F13%2Fscrapydajianxiangmu%2F</url>
    <content type="text"><![CDATA[1.输入workon命令，进入已经设置好的一个虚拟环境。 2.安装scrapy框架:pip install scrapy 3.遇到如下报错信息，进入https://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy 下载Twisted库的wheel文件 4.安装Twisted 5.执行scrapy startproject 项目名称 5.using template director：是scrapy内置的一个框架模板 6.进入到项目文件夹中 7.爬虫项目文件介绍 8.通过cmd爬虫项目命令：scrapy crawl baidu(爬虫名称) 9.执行以上命令会报错只需要安装pip install pypiwin32即可解决然后再执行命令：：scrapy crawl baidu(爬虫名称)]]></content>
      <tags>
        <tag>学习心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python虚拟环境的安装和配置(windows)]]></title>
    <url>%2F2018%2F09%2F12%2Fxunihuanjing%2F</url>
    <content type="text"><![CDATA[1.下载virtualenvwrapper-win 包在黑窗中输入pip install virtualenvwrapper-win123C:\Users\Administrator&gt;pip install virtualenvwrapper-winRequirement already satisfied: virtualenvwrapper-win in c:\programdata\anaconda3\lib\site-packages (1.2.5)Requirement already satisfied: virtualenv in c:\programdata\anaconda3\lib\site-packages (from virtualenvwrapper-win) (16.0.0) 2.输入workon命令查看是否可用123Pass a name to activate one of the following virtualenvs:==============================================================================找不到文件 3.使用virtualenvwrapper创建虚拟环境在黑窗中输入mkvirtualenv py3scrapy (py3scrapy为虚拟环境名)1234C:\Users\Administrator&gt;mkvirtualenv py4scrapyUsing base prefix 'c:\\programdata\\anaconda3'New python executable in D:\Envs\py4scrapy\Scripts\python.exeInstalling setuptools, pip, wheel...done. 默认放在C:\Users\Administrator\Envs目录中可以修改存放的路径：找到系统环境变量，编辑系统变量,添加WORKON_HOME为变量名,D:\ENVS为变量值运行workon，目录中没有虚拟环境了，因为默认目录已经改变，可以将之前的虚拟环境拷贝到新目录下 新建一个虚拟环境，完成后自动进入该虚拟环境 以后再进入虚拟环境，就不需要记住安装路径了直接使用以下命令：列出虚拟环境列表：workon新建虚拟环境：mkvirtualenv [虚拟环境名称]启动/切换虚拟环境：workon [虚拟环境名称]离开虚拟环境：deactivate]]></content>
      <tags>
        <tag>学习心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[proxy_pool]]></title>
    <url>%2F2018%2F09%2F12%2Fproxy-pool%2F</url>
    <content type="text"><![CDATA[代理IP:1.免费的代理IP,可用IP少,时效性较短,大部分的代理IP可能.2.收费的代理IP,讯代理.3.爬取国外的网站,VPN代理服务器. pxory_pool:1.它将国内的代理IP网站都进行了爬取;2.代理IP爬取完毕之后,会进行金策,可用的IP会保存到数据库redis中;3.会定期将数据库中的代理IP拿出来检测,失效的IP从数据库中删除;4.支持扩展 附上proxy_pool的github链接:https://github.com/jhao104/proxy_pool]]></content>
      <tags>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[装了Chorme不用这个扩展,还不如去用IE6]]></title>
    <url>%2F2018%2F09%2F06%2Fa%2F</url>
    <content type="text"><![CDATA[这年头神器这个词已经烂大街了，但是，油猴的确配得上这个词，如果让我选择 Chrome 浏览器上只能保留一个扩展，那必须是油猴插件！这是一个可以提升上网幸福感的神级工具！说的这么邪乎？那我来告诉你这只油猴子都能干些什么吧。简单粗暴，直接上图！………..好吧,因为没有备案,所以图床还不能用…那就简单来讲讲吧,大致来说能干的事情非常的多,比如能看VIP视频、能下知网论文、磁链在线看、网盘免输密……等等,还有很多你想不到的功能.这么逆天的神器到底是个什么东西？它其实是一个脚本管理器，可以为你的浏览器加载各种各样有趣、实用、脑洞大开的脚本，对你正在访问的指定网站按照你的需求进行修改，从而让你的浏览器前所未有地强大。这款插件除了适用于 Chrome ,还支持 Microsoft Edge，Safari，Opera ，Firefox，以及使用 Chromium 内核的国产浏览器（比如QQ、猎豹、360、百度等），在 http://tampermonkey.net 这里你可以找到对应的油猴版本下载。我个人比较喜欢用Chrome,所以就以Chrome 为例讲解啦。 先安装油猴插件，最新版本号是4.4—— 1、可以科学上网的同学建议在线安装，去谷歌商店搜索 Tampermonkey 下载安装。 2、如果没有科学上网的条件，可以去 www.crx4chrome.com/crx/755/ 下载到本地，然后离线安装。顺便提一句，crx4chrome 这个站可以无需科学上网就能下载到最新版本的 Chrome 扩展，建议收藏。 安装成功之后，你会在 Chrome 的扩展栏看到一个黑色图标，证明油猴已经安装成功。 安装就这么简单，那这时候是不是就可以使用上边的那些神奇功能呢？ 还不行，安装好油猴插件相当于你手里有了一把枪，还要有子弹才能发挥枪的威力。 再送你一个丰富的弹药库，https://greasyfork.org/zh-CN/scripts，长枪短炮，应有尽有，刚才那些功能强大的脚本都来自于这个网站。 油猴上好玩好用的脚本实在是太多了，大家可以去自己慢慢淘，总有一款适合自己.]]></content>
      <tags>
        <tag>福利专区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy框架自定义pipeline]]></title>
    <url>%2F2018%2F09%2F04%2Ftest1%2F</url>
    <content type="text"><![CDATA[自定义pipeline首先,需要我们在pipeline.py文件中自定义.需要注意的是自定义的每一个pipeline必须是一个独立的Python类,也就是继承于object.就可以定义pipeline类,定义自己想要使用的储存方法. 演示保存为csv12345678910111213class NovelSpiderCsvPipeline(object): def open_spider(self,spider): self.f=open('novel,csv','w',encoding='utf-8',newline='') self.writer=csv.DictWriter(self.f,fieldnames=["novel_name","novel_info","novel_type","novel_author"]) self.writer.writeheader() def process_item(self,item,spider): item_dict=dict(item) self.writer.writerow(item_dict) return item def close_spider(self,spider): self.f.close() 然后在pipeline写好代码之后,不是就完成了,还有一些步骤要做.自定义的pipeline必须在settings中的ITEM_PIPELINES里面启用,否则该pipeline不会生效1ITEM_PIPELINES = &#123;'NovelSpider.pipelines.NovelSpiderCsvPipeline': 301&#125; 最后运行就可以了]]></content>
      <tags>
        <tag>学习心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客简介]]></title>
    <url>%2F2018%2F09%2F03%2Ftest%2F</url>
    <content type="text"><![CDATA[欢迎光临本博客的所有朋友们! 这是个普通空间，在这里故事不会被渲染，因为它来源于生活。 以前经常发表一些帖子或者通过聊天软件来表达自己的想法,可是都是零散的和杂乱的. 现在创建了一个个人博客,我可以把自己以前的和每天激发的一些想法或者感受放在自己的博客上,每次在写文章的时候,可能又会产生新的想法.虽然一些想法一些思考只是些皮毛,没有什么深度,但是当下笔去写的时候每次都会对某个小小的问题有了跟多的一点点思考.生活中每天的一个小小的事情都会引起人们的思考,甚至是和朋友的聊天中一个小小的火花迸发. 督促自己努力,把一时的想法变成观点,争取在声明中的每天里留下点什么,这也是写博客的目的之一.这也就能督促自己每一天不要浑浑噩噩,时光流逝无痕无声无息,写博客也是在自己的每一天留下了一道浅浅的思考和划痕.]]></content>
      <tags>
        <tag>生活笔记</tag>
      </tags>
  </entry>
</search>
