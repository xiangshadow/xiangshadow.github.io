<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>&amp;ensp;&amp;emsp;静觅</title>
  
  <subtitle>翔的个人网站</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.gaoxiangxiang.cn/"/>
  <updated>2018-09-28T08:29:19.921Z</updated>
  <id>http://www.gaoxiangxiang.cn/</id>
  
  <author>
    <name>翔</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Gerapy分布式爬虫管理框架</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/28/fenbukuangjia/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/28/fenbukuangjia/</id>
    <published>2018-09-28T08:15:36.000Z</published>
    <updated>2018-09-28T08:29:19.921Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、介绍："><a href="#一、介绍：" class="headerlink" title="一、介绍："></a>一、介绍：</h2><p>Gerapy 是一款分布式爬虫管理框架，支持 Python 3，基于 Scrapy、Scrapyd、Scrapyd-Client、Scrapy-Redis、Scrapyd-API、Scrapy-Splash、Jinjia2、Django、Vue.js 开发。</p><a id="more"></a><h2 id="二、gerapy的初始化配置："><a href="#二、gerapy的初始化配置：" class="headerlink" title="二、gerapy的初始化配置："></a>二、gerapy的初始化配置：</h2><h3 id="1-安装gerapy框架。"><a href="#1-安装gerapy框架。" class="headerlink" title="1.安装gerapy框架。"></a>1.安装gerapy框架。</h3><p>$ pip install gerapy</p><h3 id="2-检查gerapy是否可用。"><a href="#2-检查gerapy是否可用。" class="headerlink" title="2.检查gerapy是否可用。"></a>2.检查gerapy是否可用。</h3><p>$ gerapy<br><img src="/2018/09/28/fenbukuangjia/图片1.png" alt="你想输入的替代文字"></p><h3 id="3-初始化gerapy，生成gerapy框架的工作目录。-在哪初始化，工作目录就创建在哪。初始化完成，进入gerapy文件夹，会有一个projects文件夹。"><a href="#3-初始化gerapy，生成gerapy框架的工作目录。-在哪初始化，工作目录就创建在哪。初始化完成，进入gerapy文件夹，会有一个projects文件夹。" class="headerlink" title="3.初始化gerapy，生成gerapy框架的工作目录。(在哪初始化，工作目录就创建在哪。初始化完成，进入gerapy文件夹，会有一个projects文件夹。)"></a>3.初始化gerapy，生成gerapy框架的工作目录。(在哪初始化，工作目录就创建在哪。初始化完成，进入gerapy文件夹，会有一个projects文件夹。)</h3><p>$ gerapy init</p><h3 id="4-先进入gerapy目录，再执行gerapy数据库的初始化，建立相关的数据库表。"><a href="#4-先进入gerapy目录，再执行gerapy数据库的初始化，建立相关的数据库表。" class="headerlink" title="4.先进入gerapy目录，再执行gerapy数据库的初始化，建立相关的数据库表。"></a>4.先进入gerapy目录，再执行gerapy数据库的初始化，建立相关的数据库表。</h3><p>$ cd gerapy<br>$ gerapy migrate<br><img src="/2018/09/28/fenbukuangjia/图片2.png" alt="你想输入的替代文字"></p><h3 id="5-在gerapy目录下，启动gerapy服务，默认在8000端口。"><a href="#5-在gerapy目录下，启动gerapy服务，默认在8000端口。" class="headerlink" title="5.在gerapy目录下，启动gerapy服务，默认在8000端口。"></a>5.在gerapy目录下，启动gerapy服务，默认在8000端口。</h3><p>$ gerapy runserver<br><img src="/2018/09/28/fenbukuangjia/图片3.png" alt="你想输入的替代文字"></p><h3 id="6-打开浏览器，输入：http-localhost-8000，可以看到-Gerapy-的主界面。"><a href="#6-打开浏览器，输入：http-localhost-8000，可以看到-Gerapy-的主界面。" class="headerlink" title="6.打开浏览器，输入：http://localhost:8000，可以看到 Gerapy 的主界面。"></a>6.打开浏览器，输入：<a href="http://localhost:8000，可以看到" target="_blank" rel="noopener">http://localhost:8000，可以看到</a> Gerapy 的主界面。</h3><p><img src="/2018/09/28/fenbukuangjia/图片4.png" alt="你想输入的替代文字"></p><h3 id="7-完成以上步骤，说明gerapy初始化成功了。但是现在还没有添加主机和项目，所有的主机数量和项目数量都是0。"><a href="#7-完成以上步骤，说明gerapy初始化成功了。但是现在还没有添加主机和项目，所有的主机数量和项目数量都是0。" class="headerlink" title="7.完成以上步骤，说明gerapy初始化成功了。但是现在还没有添加主机和项目，所有的主机数量和项目数量都是0。"></a>7.完成以上步骤，说明gerapy初始化成功了。但是现在还没有添加主机和项目，所有的主机数量和项目数量都是0。</h3><h2 id="三、配置gerapy的主机"><a href="#三、配置gerapy的主机" class="headerlink" title="三、配置gerapy的主机"></a>三、配置gerapy的主机</h2><h3 id="1-点击左侧-Clients-选项卡，即主机管理页面，添加我们的-Scrapyd-远程服务，点击右上角的创建按钮即可添加我们需要管理的-Scrapyd-服务。"><a href="#1-点击左侧-Clients-选项卡，即主机管理页面，添加我们的-Scrapyd-远程服务，点击右上角的创建按钮即可添加我们需要管理的-Scrapyd-服务。" class="headerlink" title="1. 点击左侧 Clients 选项卡，即主机管理页面，添加我们的 Scrapyd 远程服务，点击右上角的创建按钮即可添加我们需要管理的 Scrapyd 服务。"></a>1. 点击左侧 Clients 选项卡，即主机管理页面，添加我们的 Scrapyd 远程服务，点击右上角的创建按钮即可添加我们需要管理的 Scrapyd 服务。</h3><p><img src="/2018/09/28/fenbukuangjia/图片5.png" alt="你想输入的替代文字"><br><img src="/2018/09/28/fenbukuangjia/图片6.png" alt="你想输入的替代文字"><br><img src="/2018/09/28/fenbukuangjia/图片7.png" alt="你想输入的替代文字"></p><h3 id="2-在cmd中，开启scrapyd服务。-如果scrapyd在远程服务器上已经部署成功了，那么是不需要再次进行开启的。一般远程服务器上的scrapyd会一直保持运行状态。"><a href="#2-在cmd中，开启scrapyd服务。-如果scrapyd在远程服务器上已经部署成功了，那么是不需要再次进行开启的。一般远程服务器上的scrapyd会一直保持运行状态。" class="headerlink" title="2.在cmd中，开启scrapyd服务。(如果scrapyd在远程服务器上已经部署成功了，那么是不需要再次进行开启的。一般远程服务器上的scrapyd会一直保持运行状态。)"></a>2.在cmd中，开启scrapyd服务。(如果scrapyd在远程服务器上已经部署成功了，那么是不需要再次进行开启的。一般远程服务器上的scrapyd会一直保持运行状态。)</h3><h3 id="3-再次刷新主机管理，scrapyd的连接状态变成normal即可。"><a href="#3-再次刷新主机管理，scrapyd的连接状态变成normal即可。" class="headerlink" title="3.再次刷新主机管理，scrapyd的连接状态变成normal即可。"></a>3.再次刷新主机管理，scrapyd的连接状态变成normal即可。</h3><p><img src="/2018/09/28/fenbukuangjia/图片8.png" alt="你想输入的替代文字"></p><h2 id="四、在gerapy中部署爬虫项目"><a href="#四、在gerapy中部署爬虫项目" class="headerlink" title="四、在gerapy中部署爬虫项目"></a>四、在gerapy中部署爬虫项目</h2><h3 id="1-点击左侧的-Projects-，即项目管理选项。"><a href="#1-点击左侧的-Projects-，即项目管理选项。" class="headerlink" title="1. 点击左侧的 Projects ，即项目管理选项。"></a>1. 点击左侧的 Projects ，即项目管理选项。</h3><p><img src="/2018/09/28/fenbukuangjia/图片9.png" alt="你想输入的替代文字"></p><h3 id="2-将自己的爬虫项目，拷贝到gerapy目录下的projects目录下。"><a href="#2-将自己的爬虫项目，拷贝到gerapy目录下的projects目录下。" class="headerlink" title="2.将自己的爬虫项目，拷贝到gerapy目录下的projects目录下。"></a>2.将自己的爬虫项目，拷贝到gerapy目录下的projects目录下。</h3><p><img src="/2018/09/28/fenbukuangjia/图片10.png" alt="你想输入的替代文字"></p><h3 id="3-刷新浏览器页面，我们便可以看到-Gerapy-检测到了这个项目。"><a href="#3-刷新浏览器页面，我们便可以看到-Gerapy-检测到了这个项目。" class="headerlink" title="3.刷新浏览器页面，我们便可以看到 Gerapy 检测到了这个项目。"></a>3.刷新浏览器页面，我们便可以看到 Gerapy 检测到了这个项目。</h3><p><img src="/2018/09/28/fenbukuangjia/图片11.png" alt="你想输入的替代文字"></p><h3 id="4-点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于-Git-的-commit-信息，然后点击打包按钮，即可发现-Gerapy-会提示打包成功，同时在左侧显示打包的结果和打包名称。"><a href="#4-点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于-Git-的-commit-信息，然后点击打包按钮，即可发现-Gerapy-会提示打包成功，同时在左侧显示打包的结果和打包名称。" class="headerlink" title="4.点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于 Git 的 commit 信息，然后点击打包按钮，即可发现 Gerapy 会提示打包成功，同时在左侧显示打包的结果和打包名称。"></a>4.点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于 Git 的 commit 信息，然后点击打包按钮，即可发现 Gerapy 会提示打包成功，同时在左侧显示打包的结果和打包名称。</h3><p><img src="/2018/09/28/fenbukuangjia/图片12.png" alt="你想输入的替代文字"></p><h3 id="5-开始打包。"><a href="#5-开始打包。" class="headerlink" title="5.开始打包。"></a>5.开始打包。</h3><p><img src="/2018/09/28/fenbukuangjia/图片13.png" alt="你想输入的替代文字"></p><h3 id="6-打包完成以后，开始将爬虫项目部署到scrapyd服务上。"><a href="#6-打包完成以后，开始将爬虫项目部署到scrapyd服务上。" class="headerlink" title="6.打包完成以后，开始将爬虫项目部署到scrapyd服务上。"></a>6.打包完成以后，开始将爬虫项目部署到scrapyd服务上。</h3><p><img src="/2018/09/28/fenbukuangjia/图片14.png" alt="你想输入的替代文字"></p><h2 id="五、开始调度爬虫，检测爬虫的运行状态。"><a href="#五、开始调度爬虫，检测爬虫的运行状态。" class="headerlink" title="五、开始调度爬虫，检测爬虫的运行状态。"></a>五、开始调度爬虫，检测爬虫的运行状态。</h2><h3 id="1-部署完毕之后就可以回到-“主机管理”页面进行任务调度。"><a href="#1-部署完毕之后就可以回到-“主机管理”页面进行任务调度。" class="headerlink" title="1.部署完毕之后就可以回到 “主机管理”页面进行任务调度。"></a>1.部署完毕之后就可以回到 “主机管理”页面进行任务调度。</h3><p><img src="/2018/09/28/fenbukuangjia/图片15.png" alt="你想输入的替代文字"></p><h3 id="2-选择要运行的爬虫项目。"><a href="#2-选择要运行的爬虫项目。" class="headerlink" title="2.选择要运行的爬虫项目。"></a>2.选择要运行的爬虫项目。</h3><p><img src="/2018/09/28/fenbukuangjia/图片16.png" alt="你想输入的替代文字"></p><h3 id="3-查看运行结果。"><a href="#3-查看运行结果。" class="headerlink" title="3.查看运行结果。"></a>3.查看运行结果。</h3><p><img src="/2018/09/28/fenbukuangjia/图片17.png" alt="你想输入的替代文字"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、介绍：&quot;&gt;&lt;a href=&quot;#一、介绍：&quot; class=&quot;headerlink&quot; title=&quot;一、介绍：&quot;&gt;&lt;/a&gt;一、介绍：&lt;/h2&gt;&lt;p&gt;Gerapy 是一款分布式爬虫管理框架，支持 Python 3，基于 Scrapy、Scrapyd、Scrapyd-Client、Scrapy-Redis、Scrapyd-API、Scrapy-Splash、Jinjia2、Django、Vue.js 开发。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术分享" scheme="http://www.gaoxiangxiang.cn/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>scrapyd部署爬虫项目</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/27/bushupachong/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/27/bushupachong/</id>
    <published>2018-09-27T13:12:27.000Z</published>
    <updated>2018-09-28T08:37:07.869Z</updated>
    
    <content type="html"><![CDATA[<p>它就相当于是一个服务器，用于将自己本地的爬虫代码，打包上传到服务器上，让这个爬虫在服务器上运行，可以实现对爬虫的远程管理。(远程启动爬虫，远程关闭爬虫，远程查看爬虫的一些日志。)</p><a id="more"></a><h2 id="1-Scrapd的安装。"><a href="#1-Scrapd的安装。" class="headerlink" title="1.Scrapd的安装。"></a>1.Scrapd的安装。</h2><p>Pip install scrapyd</p><h2 id="2-如何将本地的爬虫项目Deploying-打包-，上传至scrapyd这个服务中。"><a href="#2-如何将本地的爬虫项目Deploying-打包-，上传至scrapyd这个服务中。" class="headerlink" title="2.如何将本地的爬虫项目Deploying(打包)，上传至scrapyd这个服务中。"></a>2.如何将本地的爬虫项目Deploying(打包)，上传至scrapyd这个服务中。</h2><p>a&gt;scrapyd提供了一个客户端工具，就是scrapyd-client，使用这个工具对scrapyd这个服务进行操作，比如：向scrapyd服务打包上传项目。scrapyd-client类似于redis-cli.exe、mongodb数据库的client。<br>scrapyd-client下载地址：<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd-client</a></p><p>b&gt;Pip install scrapyd-client==1.2.0a1</p><p>注意：服务端scrapyd(==1.2)和客户端scrapyd-client(==1.1)安装的版本一定要保持一致。</p><h2 id="3-上述服务和客户端安装好之后，可以启动scrapyd这个服务了，服务启动之后，不要关闭。"><a href="#3-上述服务和客户端安装好之后，可以启动scrapyd这个服务了，服务启动之后，不要关闭。" class="headerlink" title="3.上述服务和客户端安装好之后，可以启动scrapyd这个服务了，服务启动之后，不要关闭。"></a>3.上述服务和客户端安装好之后，可以启动scrapyd这个服务了，服务启动之后，不要关闭。</h2><p><img src="/2018/09/27/bushupachong/图片1.png" alt="你想输入的替代文字"></p><p>访问127.0.0.1:6800,出现以下页面表示成功启动scrapyd服务：<br><img src="/2018/09/27/bushupachong/图片2.png" alt="你想输入的替代文字"></p><h2 id="4-gt-配置爬虫项目，完成以后，再通过addversion-json进行打包。"><a href="#4-gt-配置爬虫项目，完成以后，再通过addversion-json进行打包。" class="headerlink" title="4&gt;配置爬虫项目，完成以后，再通过addversion.json进行打包。"></a>4&gt;配置爬虫项目，完成以后，再通过addversion.json进行打包。</h2><p><img src="/2018/09/27/bushupachong/图片3.png" alt="你想输入的替代文字"></p><p>修改scrapy.cfg文件：<br><img src="/2018/09/27/bushupachong/图片4.png" alt="你想输入的替代文字"></p><h2 id="5-gt-上述的scrapyd服务窗口cmd不要关闭，再新打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。"><a href="#5-gt-上述的scrapyd服务窗口cmd不要关闭，再新打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。" class="headerlink" title="5&gt;上述的scrapyd服务窗口cmd不要关闭，再新打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。"></a>5&gt;上述的scrapyd服务窗口cmd不要关闭，再新打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。</h2><h3 id="a-gt-进入项目根目录，然后输入scrapyd-deploy命令，查看scrapyd-client客户端命令能否正常使用；"><a href="#a-gt-进入项目根目录，然后输入scrapyd-deploy命令，查看scrapyd-client客户端命令能否正常使用；" class="headerlink" title="a&gt;进入项目根目录，然后输入scrapyd-deploy命令，查看scrapyd-client客户端命令能否正常使用；"></a>a&gt;进入项目根目录，然后输入scrapyd-deploy命令，查看scrapyd-client客户端命令能否正常使用；</h3><p><img src="/2018/09/27/bushupachong/图片5.png" alt="你想输入的替代文字"></p><h3 id="b-gt-查看当前可用于打包上传的爬虫项目；"><a href="#b-gt-查看当前可用于打包上传的爬虫项目；" class="headerlink" title="b&gt;查看当前可用于打包上传的爬虫项目；"></a>b&gt;查看当前可用于打包上传的爬虫项目；</h3><p><img src="/2018/09/27/bushupachong/图片6.png" alt="你想输入的替代文字"></p><h3 id="c-gt-使用scrapyd-deploy命令打包上传项目；"><a href="#c-gt-使用scrapyd-deploy命令打包上传项目；" class="headerlink" title="c&gt;使用scrapyd-deploy命令打包上传项目；"></a>c&gt;使用scrapyd-deploy命令打包上传项目；</h3><p>命令：Scrapyd-deploy  bole  -p  jobbolespider<br>参数：<br>Status: “ok”/”error” 项目上传状态<br>Project: 上传的项目名称<br>Version: 项目的版本号，值是时间戳<br>Spiders: 项目Project包含的爬虫个数<br><img src="/2018/09/27/bushupachong/图片7.png" alt="你想输入的替代文字"></p><h3 id="d-gt-通过API接口，查看已经上传至scrapyd服务的项目；"><a href="#d-gt-通过API接口，查看已经上传至scrapyd服务的项目；" class="headerlink" title="d&gt;通过API接口，查看已经上传至scrapyd服务的项目；"></a>d&gt;通过API接口，查看已经上传至scrapyd服务的项目；</h3><p>命令：curl  <a href="http://localhost:6800/listprojects.json" target="_blank" rel="noopener">http://localhost:6800/listprojects.json</a><br>键值：<br>Projects: [] 所有已经上传的爬虫项目，都会显示在这个列表中。<br><img src="/2018/09/27/bushupachong/图片8.png" alt="你想输入的替代文字"></p><h3 id="e-gt-通过API接口，查看某一个项目中的所有爬虫名称；"><a href="#e-gt-通过API接口，查看某一个项目中的所有爬虫名称；" class="headerlink" title="e&gt;通过API接口，查看某一个项目中的所有爬虫名称；"></a>e&gt;通过API接口，查看某一个项目中的所有爬虫名称；</h3><p>命令：curl  <a href="http://localhost:6800/listspiders.json?project=jobbolespider" target="_blank" rel="noopener">http://localhost:6800/listspiders.json?project=jobbolespider</a><br><img src="/2018/09/27/bushupachong/图片9.png" alt="你想输入的替代文字"></p><p>注意：如果项目上传失败，需要先将爬虫项目中打包生成的文件删除(build、project.egg-info、setup.py)，然后再重新打包上传。</p><h3 id="f-gt-通过API接口，启动爬虫项目；"><a href="#f-gt-通过API接口，启动爬虫项目；" class="headerlink" title="f&gt;通过API接口，启动爬虫项目；"></a>f&gt;通过API接口，启动爬虫项目；</h3><p>命令：curl <a href="http://localhost:6800/schedule.json" target="_blank" rel="noopener">http://localhost:6800/schedule.json</a> -d project=爬虫项目名称 -d  spider=项目中某一个爬虫名称<br>键值：<br>Jobid: 是根据项目(jobbolespider)和爬虫(bole)生成的一个id，将来用于取消爬虫任务。<br><img src="/2018/09/27/bushupachong/图片10.png" alt="你想输入的替代文字"></p><h3 id="g-gt-如果上传的项目无法运行，在本地调整代码以后，需要重新打包上传。将失效的项目删除。"><a href="#g-gt-如果上传的项目无法运行，在本地调整代码以后，需要重新打包上传。将失效的项目删除。" class="headerlink" title="g&gt;如果上传的项目无法运行，在本地调整代码以后，需要重新打包上传。将失效的项目删除。"></a>g&gt;如果上传的项目无法运行，在本地调整代码以后，需要重新打包上传。将失效的项目删除。</h3><p>命令：curl <a href="http://localhost:6800/delproject.json" target="_blank" rel="noopener">http://localhost:6800/delproject.json</a> -d project=jobbolespider<br><img src="/2018/09/27/bushupachong/图片11.png" alt="你想输入的替代文字"></p><h3 id="h-gt-通过API接口，取消爬虫任务；"><a href="#h-gt-通过API接口，取消爬虫任务；" class="headerlink" title="h&gt;通过API接口，取消爬虫任务；"></a>h&gt;通过API接口，取消爬虫任务；</h3><p>参数：<br>Jobid：启动爬虫的时候分配的<br><img src="/2018/09/27/bushupachong/图片12.png" alt="你想输入的替代文字"><br><img src="/2018/09/27/bushupachong/图片13.png" alt="你想输入的替代文字"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;它就相当于是一个服务器，用于将自己本地的爬虫代码，打包上传到服务器上，让这个爬虫在服务器上运行，可以实现对爬虫的远程管理。(远程启动爬虫，远程关闭爬虫，远程查看爬虫的一些日志。)&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术分享" scheme="http://www.gaoxiangxiang.cn/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>分布式爬虫的配置</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/26/fenbushi/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/26/fenbushi/</id>
    <published>2018-09-26T09:17:01.000Z</published>
    <updated>2018-09-27T03:57:02.279Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分布式爬虫："><a href="#分布式爬虫：" class="headerlink" title="分布式爬虫："></a>分布式爬虫：</h2><pre><code>将一个项目拷贝到多台电脑上，同时爬取数据。1. 必须保证所有电脑上的代码是相同的配置。2. 在其中一台电脑上启动 redis 和 MySQL 的数据库服务。3. 同时将所有的爬虫项目运行起来。4. 在启动 redis 和 MySQL 的电脑上， 向 redis 中添加起始的 url。</code></pre><p>q = queue()<br>url = q.get() # 如果队列是空的，那么 get() 方法会一直阻塞，直到能取到 url，才会继续向下执行。</p><a id="more"></a><h2 id="单机爬虫："><a href="#单机爬虫：" class="headerlink" title="单机爬虫："></a>单机爬虫：</h2><p>一台电脑运行一个项目。去重采用了 set() 和 queue()，但是这两个都是在内存中存在的。<br>1 &gt; 其他电脑是无法获取另外一台电脑内存中的数据的。<br>2 &gt; 程序终止，内存消失。</p><h2 id="分布式问题："><a href="#分布式问题：" class="headerlink" title="分布式问题："></a>分布式问题：</h2><pre><code>1. 多台电脑如何统一的对 URL 进行去重？2. 多台电脑之间如何共用相同的队列？多台电脑获取的 request，如何在多台电脑之间进行同步？3. 多台电脑运行同一个爬虫项目，如果有机器爬虫意外终止，如何保证可以继续从队列中获取新的 request，而不是从头开始爬取？</code></pre><p>前两个问题可以基于 Redis 实现。相当于将 set() 和 queue() 从 scrapy 框架中抽离出来，将其保存在一个公共的平台中（Redis）。<br>第三个问题：scrapy_redis 已经实现了，重启爬虫不会从头开始重新爬取，而是会继续从队列中获取 request。不用担心爬虫意外终止。</p><h2 id="scrapy-redis-第三方库实现分布的部署："><a href="#scrapy-redis-第三方库实现分布的部署：" class="headerlink" title="scrapy_redis 第三方库实现分布的部署："></a>scrapy_redis 第三方库实现分布的部署：</h2><p>分布式爬虫：只需要在众多电脑中，选择其中一台开启 redis 服务，目的就是在 redis 中创建公用的 queue 和公用的 set，然后剩余电脑只需要连接 redis 服务即可，剩余电脑不需要开启 redis-server 服务。</p><h2 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h2><p>1 &gt; 在虚拟环境中安装 pip install redis<br>2 &gt; 去 github 上搜索 scrapy_redis 库，解压，保存到项目根目录下。根据提供的用例，配置我们的项目，大致三部分：<br>    1.settings.py 文件；</p><pre><code># 配置 scrapy_redis 第三方库 # 所有电脑配置调度器，这个调度器重写了 scrapy 框架内置的调度器。SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;# 所有电脑配置去重， 这个也是重写了 scrapy 内置的去重。DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</code></pre><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr"># 所有电脑配置redis的连接地址，设置局域网IP或者公网IP，保证所有电脑都能连接到同一个redis。6379</span>是redis的默认端口号。</span><br><span class="line"><span class="attr"># redis数据库默认只允许本地连接localhost，如需配置远程连接，需要修改redis的配置文件。修改完成以后，要重启redis-server服务。</span></span><br><span class="line"><span class="attr"># 1</span>. <span class="comment">(A B C)</span> 如果用的是局域网部署的分布式，选择其中一台电脑<span class="comment">(A)</span>开启redis-server服务，REDIS_URL就配置成A电脑的局域网IP地址。如果B电脑开启redis-server-&gt;REDIS_URL B电脑的IP地址。</span><br><span class="line"><span class="attr"># 2</span>. <span class="comment">(A B C)</span> 如果用的是公网IP<span class="comment">(阿里云)</span>，那么所有电脑都不需要开启redis-server服务，只需要将REDIS_URL的主机地址配置成公网IP即可。</span><br></pre></td></tr></table></figure><pre><code># myroot: 自定义的 redis 链接（可不写）。IP：开启 redis-server 服务的这台电脑的 IPREDIS_URL = &apos;redis://myroot:@192.168.40.217:6379&apos;# item_pipelines可以配置，也可以不用配置。如果配置的话：所有下载的item，除了会被保存在数据库MySQL中，还会被保存在Redis数据库中。没有配置：所有的item不会存储在Redis数据库中ITEM_PIPELINES = {    &apos;scrapy_redis.pipelines.RedisPipeline&apos;: 300}  </code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置HOST为局域网IP，或者公网IPMYSQL_HOST = <span class="string">'192.168.70.246'</span>MYSQL_DB = <span class="string">'jobbole'</span>MYSQL_USER = <span class="string">'myroot'</span>MYSQL_PASSWD = <span class="string">'123456'</span>MYSQL_CHARSET = <span class="string">'utf8'</span>MYSQL_PORT = 3306 </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置scrapy_redis第三方库<span class="comment"># 所有电脑配置调度器，这个调度器重写了scrapy框架内置的调度器。</span></span></span><br><span class="line">SCHEDULER = "scrapy_redis.scheduler.Scheduler"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 所有电脑配置去重， 这个也是重写了scrapy内置的去重。</span></span><br><span class="line">DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 所有电脑配置redis的连接地址，设置局域网IP或者公网IP，保证所有电脑都能连接到同一个redis。6379是redis的默认端口号。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis数据库默认只允许本地连接localhost，如需配置远程连接，需要修改redis的配置文件。修改完成以后，要重启redis-server服务。 </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1. (A B C) 如果用的是局域网部署的分布式，选择其中一台电脑(A)开启redis-server服务，REDIS_URL就配置成A电脑的局域网IP地址。如果B电脑开启redis-server-&gt;REDIS_URL B电脑的IP地址。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. (A B C) 如果用的是公网IP(阿里云)，那么所有电脑都不需要开启redis-server服务，只需要将REDIS_URL的主机地址配置成公网IP即可。 REDIS_URL = <span class="string">'redis://@192.168.70.246:6379'</span>  </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以配置，也可以不用配置。如果配置的话：所有下载的item，除了会被保存在数据库MySQL中，还会被保存在Redis数据库中。没有配置：所有的item不会存储在Redis数据库中。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ITEM_PIPELINES = &#123;<span class="comment">#     'scrapy_redis.pipelines.RedisPipeline': 300# &#125;</span></span></span><br></pre></td></tr></table></figure><pre><code>2.jobbole.py 文件；from scrapy_redis.spiders import RedisSpiderclass JobboleSpider(RedisSpider):    name = &apos;jobbole&apos;    allowed_domains = [&apos;jobbole.com&apos;]    # start_urls = [&apos;http://blog.jobbole.com/all-posts/&apos;]# 添加键    redis_key = &apos;jobbole:start_urls&apos;</code></pre><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy,re</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> JobbolespiderItem</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisSpider</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> BoleSpider(RedisSpider):</span><br><span class="line">    name = <span class="string">'bole'</span></span><br><span class="line">    allowed_domains = [<span class="string">'jobbole.com'</span>]</span><br><span class="line">    # start_urls = [<span class="string">'http://blog.jobbole.com/all-posts/'</span>]</span><br><span class="line"></span><br><span class="line">    # 分布式就不需要再设置起始的url，需要通过redis进行添加起始的url。起始的url添加到哪去了？被添加到了公共队列queue中。让多台机器中的其中一台从公共的reids队列queue中，获取起始的url，并对这个起始的url进行请求和解析，获取更多的url，然后将所有的url构造成Request，还放入公共队列queue中，让其他机器获取这些Request请求。</span><br><span class="line"></span><br><span class="line">    redis_key = <span class="string">'jobbole:start_urls'</span></span><br></pre></td></tr></table></figure><h2 id="3-有关数据库部分；"><a href="#3-有关数据库部分；" class="headerlink" title="3. 有关数据库部分；"></a>3. 有关数据库部分；</h2><pre><code>首先要把 MySQL 添加到环境变量中第一步：通过 mysql -uroot -p 登录 MySQL 服务。第二步：通过 grant all privileges on *.*  to &apos;myroot&apos;@&apos;%&apos; identified by &apos;123456&apos;;(注意一定要带上分号)。# *.* 表示所有数据库中的所有表，都能够被远程连接# &apos;%&apos; 表示任意 IP 都可以进行链接# &apos;myroot&apos; 具有远程链接权限的用户名，自定义。之后就使用这个 User 进行链接数据库mysql-&gt;grant all privileges on *.*  to &apos;myroot&apos;@&apos;%&apos; identified by &apos;123456&apos;;  回车即可第三步：再去修改爬虫项目（settings.py）中有关数据库的配置。登录 MySQL 服务mysql -uroot -p 创建远程连接新用户：create user &quot;user&quot;@&quot;%&quot; identified by &quot;123456&quot;;给用户分配所有数据库的访问权限：GRANT ALL privileges ON *.* TO &apos;user&apos;@&apos;%&apos;;<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment"># 配置数据库</span></span><br><span class="line"><span class="attr">MYSQL_HOST</span> = <span class="string">'192.168.53.64'</span></span><br><span class="line"><span class="attr">MYSQL_DB</span> = <span class="string">'jobbole'</span></span><br><span class="line"><span class="attr">MYSQL_USER</span> = <span class="string">'myuser'</span></span><br><span class="line"><span class="attr">MYSQL_PASSWD</span> = <span class="string">'123456'</span></span><br><span class="line"><span class="attr">MYSQL_PORT</span> = <span class="number">3306</span></span><br><span class="line"><span class="attr">MYSQL_CHARSET</span> = <span class="string">'utf8'</span></span><br></pre></td></tr></table></figure>然后在数据库中新建连接，连接名随便起，主机名或 IP 地址要与 settings.py 中配置的一致，即开启 redis-server 服务的这台电脑      的 IP，用户名和密码与 settings.py 一致。3 &gt; 将配置好的项目，拷贝到不同的机器中；4 &gt; 选择其中一台机器，开启 redis-server 服务，并修改 redis.windows.conf 配置文件： #配置远程 IP 地址，供其他的电脑进行连接 redisbind: (当前电脑 IP) 192.168.40.217 # 关闭 redis 保护模式protected-mode: no 在修改redis.windows.conf配置文件以后，必须将 &quot;Redis&quot; 服务从计算机-管理中删除，然后重新根据修改后redis.windows.conf配置文件进行注册新的服务，这样修改的配置文件才生效。 删除服务: src detele Redis 启用服务: redis-server.exe --service-install redis.windows.conf --loglevel verbose 5 &gt; 其中一台电脑启动 redis-server 服务6 &gt; 让所有爬虫项目都运行起来，由于没有起始的 url，所有爬虫会暂时处于停滞状态7 &gt; 所有爬虫都启动之后，部署 redis-server 服务的电脑，通过命令 redis-cli lpush jobbole:start_urls http://blog.jobbole.com/all-posts/ 向 redis 的 queue 中添加起始的 url输入:redis-cli.exe -h 192.168....再添加: lpush jobbole:start_urls http://blog.jobbole.com/all-posts/8 &gt; 所有爬虫开始运行，爬取数据，同时所有的数据都会保存到该爬虫所连接的远程数据库以及远程 redis 中</code></pre><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;分布式爬虫：&quot;&gt;&lt;a href=&quot;#分布式爬虫：&quot; class=&quot;headerlink&quot; title=&quot;分布式爬虫：&quot;&gt;&lt;/a&gt;分布式爬虫：&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;将一个项目拷贝到多台电脑上，同时爬取数据。
1. 必须保证所有电脑上的代码是相同的配置。
2. 在其中一台电脑上启动 redis 和 MySQL 的数据库服务。
3. 同时将所有的爬虫项目运行起来。
4. 在启动 redis 和 MySQL 的电脑上， 向 redis 中添加起始的 url。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;q = queue()&lt;br&gt;url = q.get() # 如果队列是空的，那么 get() 方法会一直阻塞，直到能取到 url，才会继续向下执行。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术分享" scheme="http://www.gaoxiangxiang.cn/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy搭建爬虫项目</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/13/scrapydajianxiangmu/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/13/scrapydajianxiangmu/</id>
    <published>2018-09-13T01:07:16.000Z</published>
    <updated>2018-09-26T10:59:42.477Z</updated>
    
    <content type="html"><![CDATA[<p>1.输入workon命令，进入已经设置好的一个虚拟环境。</p><p>2.安装scrapy框架:pip install scrapy</p><a id="more"></a><p>3.遇到如下报错信息，进入<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy" target="_blank" rel="noopener">https://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy</a> 下载Twisted库的wheel文件<img src="/2018/09/13/scrapydajianxiangmu/图片1.png" alt="你想输入的替代文字"></p><p>4.安装Twisted</p><p>5.执行scrapy startproject 项目名称<br><img src="/2018/09/13/scrapydajianxiangmu/图片2.png" alt="你想输入的替代文字"></p><p>5.using template director：是scrapy内置的一个框架模板<br><img src="/2018/09/13/scrapydajianxiangmu/图片3.png" alt="你想输入的替代文字"></p><p>6.进入到项目文件夹中<br><img src="/2018/09/13/scrapydajianxiangmu/图片4.png" alt="你想输入的替代文字"></p><p>7.爬虫项目文件介绍<br><img src="/2018/09/13/scrapydajianxiangmu/图片5.png" alt="你想输入的替代文字"></p><p>8.通过cmd爬虫项目<br>命令：scrapy  crawl  baidu(爬虫名称)</p><p>9.执行以上命令会报错<br><img src="/2018/09/13/scrapydajianxiangmu/图片6.png" alt="你想输入的替代文字"><br>只需要安装pip install pypiwin32即可解决<br>然后再执行命令：：scrapy  crawl  baidu(爬虫名称)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1.输入workon命令，进入已经设置好的一个虚拟环境。&lt;/p&gt;
&lt;p&gt;2.安装scrapy框架:pip install scrapy&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习心得" scheme="http://www.gaoxiangxiang.cn/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/"/>
    
  </entry>
  
  <entry>
    <title>Python虚拟环境的安装和配置(windows)</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/12/xunihuanjing/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/12/xunihuanjing/</id>
    <published>2018-09-12T14:46:37.000Z</published>
    <updated>2018-09-13T00:54:50.165Z</updated>
    
    <content type="html"><![CDATA[<p class="description"></p><a id="more"></a><h1 id="1-下载virtualenvwrapper-win-包"><a href="#1-下载virtualenvwrapper-win-包" class="headerlink" title="1.下载virtualenvwrapper-win 包"></a>1.下载virtualenvwrapper-win 包</h1><p>在黑窗中输入pip install virtualenvwrapper-win<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">C:</span>\Users\Administrator&gt;pip install virtualenvwrapper-win</span><br><span class="line">Requirement already <span class="symbol">satisfied:</span> virtualenvwrapper-win in <span class="symbol">c:</span>\programdata\anaconda3\<span class="class"><span class="keyword">lib</span>\<span class="title">site</span>-<span class="title">packages</span> (1.2.5)</span></span><br><span class="line">Requirement already <span class="symbol">satisfied:</span> virtualenv in <span class="symbol">c:</span>\programdata\anaconda3\<span class="class"><span class="keyword">lib</span>\<span class="title">site</span>-<span class="title">packages</span> (<span class="title">from</span> <span class="title">virtualenvwrapper</span>-<span class="title">win</span>) (16.0.0)</span></span><br></pre></td></tr></table></figure></p><h1 id="2-输入workon命令查看是否可用"><a href="#2-输入workon命令查看是否可用" class="headerlink" title="2.输入workon命令查看是否可用"></a>2.输入workon命令查看是否可用</h1><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Pass a name to activate one of the following virtualenvs:</span><br><span class="line">==============================================================================</span><br><span class="line">找不到文件</span><br></pre></td></tr></table></figure><h1 id="3-使用virtualenvwrapper创建虚拟环境"><a href="#3-使用virtualenvwrapper创建虚拟环境" class="headerlink" title="3.使用virtualenvwrapper创建虚拟环境"></a>3.使用virtualenvwrapper创建虚拟环境</h1><p>在黑窗中输入mkvirtualenv py3scrapy  (py3scrapy为虚拟环境名)<br><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">C:<span class="symbol">\U</span>sers<span class="symbol">\A</span>dministrator&gt;mkvirtualenv py4scrapy</span><br><span class="line">Using base prefix 'c:<span class="symbol">\\</span>programdata<span class="symbol">\\</span>anaconda3'</span><br><span class="line">New python executable in D:<span class="symbol">\E</span>nvs<span class="symbol">\p</span>y4scrapy<span class="symbol">\S</span>cripts<span class="symbol">\p</span>ython.exe</span><br><span class="line">Installing setuptools, pip, wheel...done.</span><br></pre></td></tr></table></figure></p><p>默认放在C:\Users\Administrator\Envs目录中<br>可以修改存放的路径：<br>找到系统环境变量，编辑系统变量,添加WORKON_HOME为变量名,D:\ENVS为变量值<br>运行workon，目录中没有虚拟环境了，因为默认目录已经改变，可以将之前的虚拟环境拷贝到新目录下</p><p>新建一个虚拟环境，完成后自动进入该虚拟环境</p><p>以后再进入虚拟环境，就不需要记住安装路径了直接使用以下命令：<br>列出虚拟环境列表：workon<br>新建虚拟环境：mkvirtualenv [虚拟环境名称]<br>启动/切换虚拟环境：workon [虚拟环境名称]<br>离开虚拟环境：deactivate</p><hr>]]></content>
    
    <summary type="html">
    
      “虚拟环境” 来源于 ShadoWin 虚拟软件所实现的功能。ShadoWin 的工作原理是以专利的实时动态程序行为修饰与模拟算法，直接利用本机的 OS，模拟出自带与本机相容 OS 的虚拟机（Vista 下可模拟 Vista、XP，Windows 7 下则可模拟 Windows 7、Vista、XP），也称为 “虚拟环境”。
    
    </summary>
    
    
      <category term="学习心得" scheme="http://www.gaoxiangxiang.cn/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/"/>
    
  </entry>
  
  <entry>
    <title>proxy_pool</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/12/proxy-pool/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/12/proxy-pool/</id>
    <published>2018-09-12T06:14:35.000Z</published>
    <updated>2018-09-12T06:52:38.967Z</updated>
    
    <content type="html"><![CDATA[<p class="description"></p><a id="more"></a><h2 id="代理IP"><a href="#代理IP" class="headerlink" title="代理IP:"></a>代理IP:</h2><p>1.免费的代理IP,可用IP少,时效性较短,大部分的代理IP可能.<br>2.收费的代理IP,讯代理.<br>3.爬取国外的网站,VPN代理服务器.</p><h2 id="pxory-pool"><a href="#pxory-pool" class="headerlink" title="pxory_pool:"></a>pxory_pool:</h2><p>1.它将国内的代理IP网站都进行了爬取;<br>2.代理IP爬取完毕之后,会进行金策,可用的IP会保存到数据库redis中;<br>3.会定期将数据库中的代理IP拿出来检测,失效的IP从数据库中删除;<br>4.支持扩展</p><p>附上proxy_pool的github链接:<a href="https://github.com/jhao104/proxy_pool" target="_blank" rel="noopener">https://github.com/jhao104/proxy_pool</a></p><hr>]]></content>
    
    <summary type="html">
    
      代理服务器（ProxyServer）是一种重要的服务器安全功能，它的工作主要在开放系统互联 (OSI) 模型的会话层，从而起到防火墙的作用。代理服务器大多被用来连接INTERNET（国际互联网）和 Local Area Network（局域网）。
    
    </summary>
    
    
      <category term="代理" scheme="http://www.gaoxiangxiang.cn/tags/%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>装了Chorme不用这个扩展,还不如去用IE6</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/06/a/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/06/a/</id>
    <published>2018-09-06T01:40:28.000Z</published>
    <updated>2018-09-12T11:10:26.699Z</updated>
    
    <content type="html"><![CDATA[<p class="description"></p><a id="more"></a><p>这年头神器这个词已经烂大街了，但是，油猴的确配得上这个词，如果让我选择 Chrome 浏览器上只能保留一个扩展，那必须是油猴插件！这是一个可以提升上网幸福感的神级工具！<br>说的这么邪乎？那我来告诉你这只油猴子都能干些什么吧。简单粗暴，直接上图！<br>………..<br>好吧,因为没有备案,所以图床还不能用…<br>那就简单来讲讲吧,大致来说能干的事情非常的多,比如能看VIP视频、能下知网论文、磁链在线看、网盘免输密……<br>等等,还有很多你想不到的功能.<br>这么逆天的神器到底是个什么东西？它其实是一个脚本管理器，可以为你的浏览器加载各种各样有趣、实用、脑洞大开的脚本，对你正在访问的指定网站按照你的需求进行修改，从而让你的浏览器前所未有地强大。<br>这款插件除了适用于 Chrome ,还支持 Microsoft Edge，Safari，Opera ，Firefox，以及使用 Chromium 内核的国产浏览器（比如QQ、猎豹、360、百度等），在 <a href="http://tampermonkey.net" target="_blank" rel="noopener">http://tampermonkey.net</a> 这里你可以找到对应的油猴版本下载。我个人比较喜欢用Chrome,所以就以Chrome 为例讲解啦。</p><p>先安装油猴插件，最新版本号是4.4——</p><p>1、可以科学上网的同学建议在线安装，去谷歌商店搜索 Tampermonkey 下载安装。</p><p>2、如果没有科学上网的条件，可以去 <a href="http://www.crx4chrome.com/crx/755/" target="_blank" rel="noopener">www.crx4chrome.com/crx/755/</a> 下载到本地，然后离线安装。顺便提一句，crx4chrome 这个站可以无需科学上网就能下载到最新版本的 Chrome 扩展，建议收藏。</p><p>安装成功之后，你会在 Chrome 的扩展栏看到一个黑色图标，证明油猴已经安装成功。</p><p>安装就这么简单，那这时候是不是就可以使用上边的那些神奇功能呢？</p><p>还不行，安装好油猴插件相当于你手里有了一把枪，还要有子弹才能发挥枪的威力。</p><p>再送你一个丰富的弹药库，<a href="https://greasyfork.org/zh-CN/scripts，长枪短炮，应有尽有，刚才那些功能强大的脚本都来自于这个网站。" target="_blank" rel="noopener">https://greasyfork.org/zh-CN/scripts，长枪短炮，应有尽有，刚才那些功能强大的脚本都来自于这个网站。</a></p><p>油猴上好玩好用的脚本实在是太多了，大家可以去自己慢慢淘，总有一款适合自己.</p><hr>]]></content>
    
    <summary type="html">
    
      插件 (Plug-in,又译外挂) 是一种遵循一定规范的应用程序接口编写出来的程序。其只能运行在程序规定的系统平台下（可能同时支持多个平台），而不能脱离指定的平台单独运行。插件的定位是开发实现原纯净系统平台、应用软件平台不具备的功能的程序
    
    </summary>
    
    
      <category term="福利专区" scheme="http://www.gaoxiangxiang.cn/tags/%E7%A6%8F%E5%88%A9%E4%B8%93%E5%8C%BA/"/>
    
  </entry>
  
  <entry>
    <title>scrapy框架自定义pipeline</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/04/test1/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/04/test1/</id>
    <published>2018-09-04T03:31:01.000Z</published>
    <updated>2018-09-12T11:10:41.628Z</updated>
    
    <content type="html"><![CDATA[<h2 id="自定义pipeline"><a href="#自定义pipeline" class="headerlink" title="自定义pipeline"></a>自定义pipeline</h2><p>首先,需要我们在pipeline.py文件中自定义.需要注意的是自定义的每一个pipeline必须是一个独立的Python类,也就是继承于object.就可以定义pipeline类,定义自己想要使用的储存方法.</p><a id="more"></a><h2 id="演示保存为csv"><a href="#演示保存为csv" class="headerlink" title="演示保存为csv"></a>演示保存为csv</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NovelSpiderCsvPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>,spider)</span></span><span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.f=open(<span class="string">'novel,csv'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>,newline=<span class="string">''</span>)</span><br><span class="line">            <span class="keyword">self</span>.writer=csv.DictWriter(<span class="keyword">self</span>.f,fieldnames=[<span class="string">"novel_name"</span>,<span class="string">"novel_info"</span>,<span class="string">"novel_type"</span>,<span class="string">"novel_author"</span>])</span><br><span class="line">            <span class="keyword">self</span>.writer.writeheader()</span><br><span class="line">    </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>,item,spider)</span></span><span class="symbol">:</span></span><br><span class="line">            item_dict=dict(item)</span><br><span class="line">            <span class="keyword">self</span>.writer.writerow(item_dict)</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>,spider)</span></span><span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.f.close()</span><br></pre></td></tr></table></figure><p>然后在pipeline写好代码之后,不是就完成了,还有一些步骤要做.自定义的pipeline必须在settings中的ITEM_PIPELINES里面启用,否则该pipeline不会生效<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ITEM_PIPELINES</span> = &#123;<span class="string">'NovelSpider.pipelines.NovelSpiderCsvPipeline'</span>: <span class="number">301</span>&#125;</span><br></pre></td></tr></table></figure></p><p>最后运行就可以了</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;自定义pipeline&quot;&gt;&lt;a href=&quot;#自定义pipeline&quot; class=&quot;headerlink&quot; title=&quot;自定义pipeline&quot;&gt;&lt;/a&gt;自定义pipeline&lt;/h2&gt;&lt;p&gt;首先,需要我们在pipeline.py文件中自定义.需要注意的是自定义的每一个pipeline必须是一个独立的Python类,也就是继承于object.就可以定义pipeline类,定义自己想要使用的储存方法.&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习心得" scheme="http://www.gaoxiangxiang.cn/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/"/>
    
  </entry>
  
  <entry>
    <title>博客简介</title>
    <link href="http://www.gaoxiangxiang.cn/2018/09/03/test/"/>
    <id>http://www.gaoxiangxiang.cn/2018/09/03/test/</id>
    <published>2018-09-03T01:40:28.000Z</published>
    <updated>2018-09-12T11:15:40.948Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎光临本博客的所有朋友们!</p><p>这是个普通空间，在这里故事不会被渲染，因为它来源于生活。</p><a id="more"></a><p>以前经常发表一些帖子或者通过聊天软件来表达自己的想法,可是都是零散的和杂乱的.</p><p>现在创建了一个个人博客,我可以把自己以前的和每天激发的一些想法或者感受放在自己的博客上,每次在写文章的时候,可能又会产生新的想法.虽然一些想法一些思考只是些皮毛,没有什么深度,但是当下笔去写的时候每次都会对某个小小的问题有了跟多的一点点思考.生活中每天的一个小小的事情都会引起人们的思考,甚至是和朋友的聊天中一个小小的火花迸发.</p><p>督促自己努力,把一时的想法变成观点,争取在声明中的每天里留下点什么,这也是写博客的目的之一.这也就能督促自己每一天不要浑浑噩噩,时光流逝无痕无声无息,写博客也是在自己的每一天留下了一道浅浅的思考和划痕.</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;欢迎光临本博客的所有朋友们!&lt;/p&gt;
&lt;p&gt;这是个普通空间，在这里故事不会被渲染，因为它来源于生活。&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活笔记" scheme="http://www.gaoxiangxiang.cn/tags/%E7%94%9F%E6%B4%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
